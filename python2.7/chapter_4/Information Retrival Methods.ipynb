{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import files\n",
    "import os\n",
    "import numpy as np\n",
    "#get titles\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "moviehtmldir = './movie/'\n",
    "moviedict = {}\n",
    "for filename in [f for f in os.listdir(moviehtmldir) if f[0]!='.']:\n",
    "    id = filename.split('.')[0]\n",
    "    f = open(moviehtmldir+'/'+filename)\n",
    "    parsed_html = BeautifulSoup(f.read())\n",
    "    try:\n",
    "       title = parsed_html.body.h1.text\n",
    "       \n",
    "    except:\n",
    "       title = 'none'\n",
    "    moviedict[id] = title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package 'stopwords' to\n",
      "[nltk_data]     /Users/andrea/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tknzr = WordPunctTokenizer()\n",
    "nltk.download('stopwords')\n",
    "stoplist = stopwords.words('english')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "def ListDocs(dirname):\n",
    "    docs = []\n",
    "    titles = []\n",
    "    for filename in [f for f in os.listdir(dirname) if str(f)[0]!='.']:\n",
    "        f = open(dirname+'/'+filename,'r')\n",
    "        id = filename.split('.')[0].split('_')[1]\n",
    "        titles.append(moviedict[id])\n",
    "        docs.append(f.read())\n",
    "    return docs,titles\n",
    "\n",
    "dir = './review_polarity/txt_sentoken/'\n",
    "pos_textreviews,pos_titles = ListDocs(dir+'pos/')\n",
    "neg_textreviews,neg_titles = ListDocs(dir+'neg/')\n",
    "tot_textreviews = pos_textreviews+neg_textreviews\n",
    "tot_titles = pos_titles+neg_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def PreprocessTfidf(texts,stoplist=[],stem=False):\n",
    "    newtexts = []\n",
    "    for text in texts:\n",
    "        if stem:\n",
    "           tmp = [w for w in tknzr.tokenize(text) if w not in stoplist]\n",
    "        else:\n",
    "           tmp = [stemmer.stem(w) for w in [w for w in tknzr.tokenize(text) if w not in stoplist]]\n",
    "        newtexts.append(' '.join(tmp))\n",
    "    return newtexts\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "processed_reviews = PreprocessTfidf(tot_textreviews,stoplist,True)\n",
    "mod_tfidf = vectorizer.fit(processed_reviews)\n",
    "vec_tfidf = mod_tfidf.transform(processed_reviews)\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(),vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 -- 39516\n",
      "  (0, 10607)\t1.0\n"
     ]
    }
   ],
   "source": [
    "#dump tf-idf into file\n",
    "import cPickle as pickle\n",
    "#print mod_tfidf.get_feature_names()\n",
    "print len(processed_reviews),'--',len(mod_tfidf.get_feature_names())\n",
    "v= mod_tfidf.transform(processed_reviews)\n",
    "#print v\n",
    "with open('vectorizer.pk', 'wb') as fin:\n",
    "      pickle.dump(mod_tfidf, fin)\n",
    "file = open(\"vectorizer.pk\",'r')\n",
    "load_tfidf =  pickle.load(file)\n",
    "        \n",
    "print load_tfidf.transform(PreprocessTfidf([' '.join(['drama'])],stoplist,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test LSA\n",
    "import gensim\n",
    "from gensim import models\n",
    "class GenSimCorpus(object):\n",
    "           def __init__(self, texts, stoplist=[],stem=False):\n",
    "               self.texts = texts\n",
    "               self.stoplist = stoplist\n",
    "               self.stem = stem\n",
    "               self.dictionary = gensim.corpora.Dictionary(self.iter_docs(texts, stoplist))\n",
    "               \n",
    "            \n",
    "           def __len__(self):\n",
    "               return len(self.texts)\n",
    "           def __iter__(self):\n",
    "               for tokens in self.iter_docs(self.texts, self.stoplist):\n",
    "                   yield self.dictionary.doc2bow(tokens)\n",
    "           def iter_docs(self,texts, stoplist):\n",
    "               for text in texts:\n",
    "                   if self.stem:\n",
    "                      yield (stemmer.stem(w) for w in [x for x in tknzr.tokenize(text) if x not in stoplist])\n",
    "                   else:\n",
    "                      yield (x for x in tknzr.tokenize(text) if x not in stoplist)\n",
    "\n",
    "corpus = GenSimCorpus(tot_textreviews,stoplist,True)\n",
    "dict_corpus = corpus.dictionary\n",
    "ntopics = 10\n",
    "lsi =  models.LsiModel(corpus, num_topics=ntopics, id2word=dict_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot words: 26132\n",
      "find\n"
     ]
    }
   ],
   "source": [
    "U = lsi.projection.u\n",
    "Sigma = np.eye(ntopics)*lsi.projection.s\n",
    "#calculate V\n",
    "V = gensim.matutils.corpus2dense(lsi[corpus], len(lsi.projection.s)).T / lsi.projection.s\n",
    "dict_words = {}\n",
    "for i in range(len(dict_corpus)):\n",
    "    dict_words[dict_corpus[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "def PreprocessDoc2Vec(text,stop=[],stem=False):\n",
    "    words = tknzr.tokenize(text)\n",
    "    if stem:\n",
    "       words_clean = [stemmer.stem(w) for w in [i.lower() for i in words if i not in stop]]\n",
    "    else:\n",
    "       words_clean = [i.lower() for i in words if i not in stop]\n",
    "    return words_clean\n",
    "\n",
    "Review = namedtuple('Review','words tags')\n",
    "dir = './review_polarity/txt_sentoken/'\n",
    "do2vecstem = False\n",
    "reviews_pos = []\n",
    "cnt = 0\n",
    "for filename in [f for f in os.listdir(dir+'pos/') if str(f)[0]!='.']:\n",
    "    f = open(dir+'pos/'+filename,'r')\n",
    "    reviews_pos.append(Review(PreprocessDoc2Vec(f.read(),stoplist,do2vecstem),['pos_'+str(cnt)]))\n",
    "    cnt+=1\n",
    "    \n",
    "reviews_neg = []\n",
    "cnt= 0\n",
    "for filename in [f for f in os.listdir(dir+'neg/') if str(f)[0]!='.']:\n",
    "    f = open(dir+'neg/'+filename,'r')\n",
    "    reviews_neg.append(Review(PreprocessDoc2Vec(f.read(),stoplist,do2vecstem),['neg_'+str(cnt)]))\n",
    "    cnt+=1\n",
    "\n",
    "tot_reviews = reviews_pos + reviews_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n"
     ]
    }
   ],
   "source": [
    "#define doc2vec\n",
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "vec_size = 500\n",
    "model_d2v = Doc2Vec(dm=1, dm_concat=0, size=vec_size, window=10, negative=0, hs=0, min_count=1, workers=cores)\n",
    "\n",
    "#build vocab\n",
    "model_d2v.build_vocab(tot_reviews)\n",
    "#train\n",
    "numepochs= 20\n",
    "for epoch in range(numepochs):\n",
    "    try:\n",
    "        print 'epoch %d' % (epoch)\n",
    "        model_d2v.train(tot_reviews)\n",
    "        model_d2v.alpha *= 0.99\n",
    "        model_d2v.min_alpha = model_d2v.alpha\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#query\n",
    "query = ['science','future','action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim: 0.177948650457  title: No Telling (1991)\n",
      "sim: 0.177821146567  title: Total Recall (1990)\n",
      "sim: 0.173783798661  title: Time Machine, The (1960)\n",
      "sim: 0.163031796224  title: Bicentennial Man (1999)\n",
      "sim: 0.160582512878  title: Andromeda Strain, The (1971)\n"
     ]
    }
   ],
   "source": [
    "#similar tfidf\n",
    "#sparse matrix so the metrics transform into regular vectors before computing cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "query_vec = mod_tfidf.transform(PreprocessTfidf([' '.join(query)],stoplist,True))\n",
    "sims= cosine_similarity(query_vec,vec_tfidf)[0]\n",
    "indxs_sims = sims.argsort()[::-1]\n",
    "for d in list(indxs_sims)[:5]:\n",
    "    print 'sim:',sims[d],' title:',tot_titles[d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim: 4.0370254245  doc: Star Wars: Episode I - The Phantom Menace (1999)\n",
      "sim: 3.41798397445  doc: Alien&#179; (1992)\n",
      "sim: 3.41131742531  doc: Rocky Horror Picture Show, The (1975)\n",
      "sim: 2.99980957062  doc: Starship Troopers (1997)\n",
      "sim: 2.86164366049  doc: Wild Things (1998)\n"
     ]
    }
   ],
   "source": [
    "#LSA query\n",
    "def TransformWordsListtoQueryVec(wordslist,dict_words,stem=False):\n",
    "    q = np.zeros(len(dict_words.keys()))\n",
    "    for w in wordslist:\n",
    "        if stem:\n",
    "            q[dict_words[stemmer.stem(w)]]=1.\n",
    "        else:\n",
    "            q[dict_words[w]] = 1.\n",
    "    return q\n",
    "\n",
    "q = TransformWordsListtoQueryVec(query,dict_words,True)\n",
    "\n",
    "qk =   np.dot(np.dot(q,U),Sigma)\n",
    "\n",
    "sims = np.zeros(len(tot_textreviews))\n",
    "for d in range(len(V)):\n",
    "    sims[d]=np.dot(qk,V[d])\n",
    "indxs_sims = np.argsort(sims)[::-1]  \n",
    "for d in list(indxs_sims)[:5]:\n",
    "    print 'sim:',sims[d],' doc:',tot_titles[d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance: 0.129549503326   title: Lost World: Jurassic Park, The (1997)\n",
      "relevance: 0.124721623957   title: In the Heat of the Night (1967)\n",
      "relevance: 0.122562259436   title: Charlie's Angels (2000)\n",
      "relevance: 0.119273915887   title: Batman & Robin (1997)\n",
      "relevance: 0.118506141007   title: Pok&#233;mon: The Movie 2000 (2000)\n"
     ]
    }
   ],
   "source": [
    "#doc2vec query\n",
    "#force inference to get the same result\n",
    "model_d2v.random = np.random.RandomState(1)\n",
    "query_docvec = model_d2v.infer_vector(PreprocessDoc2Vec(' '.join(query),stoplist,do2vecstem))\n",
    "\n",
    "reviews_related = model_d2v.docvecs.most_similar([query_docvec], topn=5)#model_d2v.docvecs.most_similar([query_docvec], topn=3)\n",
    "for review in reviews_related:\n",
    "    print 'relevance:',review[1],'  title:',tot_titles[review[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
