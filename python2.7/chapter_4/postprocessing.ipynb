{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import files\n",
    "import os\n",
    "import numpy as np\n",
    "#get titles\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "moviehtmldir = './movie/'\n",
    "moviedict = {}\n",
    "for filename in [f for f in os.listdir(moviehtmldir) if f[0]!='.']:\n",
    "    id = filename.split('.')[0]\n",
    "    f = open(moviehtmldir+'/'+filename)\n",
    "    parsed_html = BeautifulSoup(f.read())\n",
    "    try:\n",
    "       title = parsed_html.body.h1.text\n",
    "       \n",
    "    except:\n",
    "       title = 'none'\n",
    "    moviedict[id] = title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ListDocs(dirname):\n",
    "    docs = []\n",
    "    titles = []\n",
    "    for filename in [f for f in os.listdir(dirname) if str(f)[0]!='.']:\n",
    "        f = open(dirname+'/'+filename,'r')\n",
    "        id = filename.split('.')[0].split('_')[1]\n",
    "        titles.append(moviedict[id])\n",
    "        docs.append(f.read())\n",
    "    return docs,titles\n",
    "\n",
    "dir = './review_polarity/txt_sentoken/'\n",
    "pos_textreviews,pos_titles = ListDocs(dir+'pos/')\n",
    "neg_textreviews,neg_titles = ListDocs(dir+'neg/')\n",
    "tot_textreviews = pos_textreviews+neg_textreviews\n",
    "tot_titles = pos_titles+neg_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.006*movie + 0.004*paulie + 0.003*leila + 0.003*one + 0.003*film + 0.003*shrek + 0.003*story + 0.002*get + 0.002*like + 0.002*good'), (1, u'0.014*film + 0.007*one + 0.005*like + 0.005*movie + 0.003*even + 0.003*would + 0.003*time + 0.003*good + 0.003*first + 0.002*much'), (2, u'0.011*film + 0.006*one + 0.004*movie + 0.003*like + 0.003*even + 0.002*way + 0.002*two + 0.002*batman + 0.002*character + 0.002*films'), (3, u'0.011*film + 0.006*one + 0.005*movie + 0.004*like + 0.003*time + 0.003*first + 0.003*get + 0.003*good + 0.003*would + 0.003*story'), (4, u'0.013*film + 0.007*one + 0.007*movie + 0.005*like + 0.003*story + 0.003*even + 0.003*much + 0.003*character + 0.003*time + 0.003*well'), (5, u'0.012*film + 0.008*one + 0.007*movie + 0.005*like + 0.003*good + 0.003*also + 0.003*time + 0.003*first + 0.003*would + 0.003*story'), (6, u'0.010*film + 0.007*one + 0.006*movie + 0.004*like + 0.004*story + 0.003*even + 0.003*character + 0.003*characters + 0.003*much + 0.003*time'), (7, u'0.017*film + 0.010*movie + 0.010*one + 0.006*like + 0.004*even + 0.004*good + 0.004*time + 0.003*well + 0.003*would + 0.003*characters'), (8, u'0.010*film + 0.006*one + 0.005*movie + 0.004*time + 0.004*joe + 0.003*like + 0.003*even + 0.003*life + 0.002*love + 0.002*story'), (9, u'0.011*movie + 0.009*film + 0.008*one + 0.006*like + 0.004*good + 0.004*even + 0.004*time + 0.003*would + 0.003*much + 0.003*story')]\n"
     ]
    }
   ],
   "source": [
    "#LDA\n",
    "import gensim.models\n",
    "from gensim import models\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tknzr = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "class GenSimCorpus(object):\n",
    "           def __init__(self, texts, stoplist=[],bestwords=[],stem=False):\n",
    "               self.texts = texts\n",
    "               self.stoplist = stoplist\n",
    "               self.stem = stem\n",
    "               self.bestwords = bestwords\n",
    "               self.dictionary = gensim.corpora.Dictionary(self.iter_docs(texts, stoplist))\n",
    "            \n",
    "           def __len__(self):\n",
    "               return len(self.texts)\n",
    "           def __iter__(self):\n",
    "               for tokens in self.iter_docs(self.texts, self.stoplist):\n",
    "                   yield self.dictionary.doc2bow(tokens)\n",
    "           def iter_docs(self,texts, stoplist):\n",
    "               for text in texts:\n",
    "                   if self.stem:\n",
    "                      yield (stemmer.stem(w) for w in [x for x in tknzr.tokenize(text) if x not in stoplist])\n",
    "                   else:\n",
    "                      if len(self.bestwords)>0:\n",
    "                         yield (x for x in tknzr.tokenize(text) if x in self.bestwords)\n",
    "                      else:\n",
    "                         yield (x for x in tknzr.tokenize(text) if x not in stoplist)            \n",
    "            \n",
    "num_topics = 10\n",
    "corpus = GenSimCorpus(tot_textreviews, stoplist,[],False)\n",
    "dict_lda = corpus.dictionary\n",
    "lda = models.LdaModel(corpus, num_topics=num_topics, id2word=dict_lda,passes=10, iterations=50)\n",
    "print lda.show_topics(num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "#filter out very common words like mobie and film or very unfrequent terms\n",
    "out_ids = [tokenid for tokenid, docfreq in dict_lda.dfs.iteritems() if docfreq > 1000 or docfreq < 3 ]\n",
    "dict_lfq = copy.deepcopy(dict_lda)\n",
    "dict_lfq.filter_tokens(out_ids)\n",
    "dict_lfq.compactify()\n",
    "corpus = [dict_lfq.doc2bow(tknzr.tokenize(text)) for text in tot_textreviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic  0   words:  0.004*characters + 0.003*bad + 0.003*plot + 0.003*star + 0.003*horror + 0.003*series + 0.003*movies + 0.003*scream + 0.002*really + 0.002*know\n",
      "\n",
      "topic  1   words:  0.004*see + 0.003*people + 0.003*characters + 0.003*new + 0.003*never + 0.003*show + 0.003*know + 0.003*plot + 0.003*something + 0.002*really\n",
      "\n",
      "topic  2   words:  0.003*life + 0.003*love + 0.003*scene + 0.003*see + 0.003*director + 0.003*best + 0.003*little + 0.003*alien + 0.003*man + 0.002*films\n",
      "\n",
      "topic  3   words:  0.005*harry + 0.003*never + 0.003*carter + 0.003*wrestling + 0.003*see + 0.003*scenes + 0.003*life + 0.003*characters + 0.003*people + 0.003*williams\n",
      "\n",
      "topic  4   words:  0.003*jackie + 0.003*director + 0.003*characters + 0.003*films + 0.003*never + 0.002*action + 0.002*scene + 0.002*plot + 0.002*comedy + 0.002*could\n",
      "\n",
      "topic  5   words:  0.005*really + 0.005*bad + 0.004*see + 0.003*little + 0.003*funny + 0.003*films + 0.003*know + 0.003*could + 0.003*characters + 0.003*people\n",
      "\n",
      "topic  6   words:  0.003*action + 0.003*bad + 0.003*man + 0.003*life + 0.003*characters + 0.003*scenes + 0.002*world + 0.002*movies + 0.002*many + 0.002*could\n",
      "\n",
      "topic  7   words:  0.004*life + 0.003*little + 0.003*disney + 0.003*plot + 0.003*best + 0.002*could + 0.002*really + 0.002*man + 0.002*see + 0.002*people\n",
      "\n",
      "topic  8   words:  0.004*characters + 0.003*films + 0.003*see + 0.003*life + 0.002*scene + 0.002*best + 0.002*love + 0.002*never + 0.002*great + 0.002*people\n",
      "\n",
      "topic  9   words:  0.003*life + 0.003*scene + 0.003*see + 0.003*really + 0.003*characters + 0.003*new + 0.003*go + 0.003*people + 0.002*ryan + 0.002*love\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_lfq = models.LdaModel(corpus, num_topics=num_topics, id2word=dict_lfq,passes=10, iterations=50,alpha=0.01,eta=0.01)\n",
    "for t in range(num_topics):\n",
    "    print 'topic ',t,'  words: ',lda_lfq.print_topic(t,topn=10)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Hell (2001)\n",
      "Lumumba (2000)\n"
     ]
    }
   ],
   "source": [
    "#topics for each doc\n",
    "def GenerateDistrArrays(corpus):\n",
    "         for i,dist in enumerate(corpus[:10]):\n",
    "             dist_array = np.zeros(num_topics)\n",
    "             for d in dist:\n",
    "                 dist_array[d[0]] =d[1]\n",
    "             if dist_array.argmax() == 6 :\n",
    "                print tot_titles[i]\n",
    "corpus_lda = lda_lfq[corpus]\n",
    "GenerateDistrArrays(corpus_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package 'stopwords' to\n",
      "[nltk_data]     /Users/andrea/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tknzr = WordPunctTokenizer()\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tknzr = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stoplist = stopwords.words('english')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "def PreprocessReviews(text,stop=[],stem=False):\n",
    "    #print profile\n",
    "    words = tknzr.tokenize(text)\n",
    "    if stem:\n",
    "       words_clean = [stemmer.stem(w) for w in [i.lower() for i in words if i not in stop]]\n",
    "    else:\n",
    "       words_clean = [i.lower() for i in words if i not in stop]\n",
    "    return words_clean\n",
    "\n",
    "Review = namedtuple('Review','words title tags')\n",
    "dir = './review_polarity/txt_sentoken/'\n",
    "do2vecstem = True\n",
    "reviews_pos = []\n",
    "cnt = 0\n",
    "for filename in [f for f in os.listdir(dir+'pos/') if str(f)[0]!='.']:\n",
    "    f = open(dir+'pos/'+filename,'r')\n",
    "    id = filename.split('.')[0].split('_')[1]\n",
    "    reviews_pos.append(Review(PreprocessReviews(f.read(),stoplist,do2vecstem),moviedict[id],['pos_'+str(cnt)]))\n",
    "    cnt+=1\n",
    "    \n",
    "reviews_neg = []\n",
    "cnt= 0\n",
    "for filename in [f for f in os.listdir(dir+'neg/') if str(f)[0]!='.']:\n",
    "    f = open(dir+'neg/'+filename,'r')\n",
    "    id = filename.split('.')[0].split('_')[1]\n",
    "    reviews_neg.append(Review(PreprocessReviews(f.read(),stoplist,do2vecstem),moviedict[id],['neg_'+str(cnt)]))\n",
    "    cnt+=1\n",
    "\n",
    "tot_reviews = reviews_pos + reviews_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 - 800\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "#split in test training sets\n",
    "def word_features(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "negfeatures = [(word_features(r.words), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(word_features(r.words), 'pos') for r in reviews_pos]\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)\n",
    "print portionpos,'-',portionneg\n",
    "trainfeatures = negfeatures[:portionneg] + posfeatures[:portionpos]\n",
    "print len(trainfeatures)\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]\n",
    "#shuffle(testfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on:  400\n",
      "error rate:  0.2975\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "#training naive bayes \n",
    "classifier = NaiveBayesClassifier.train(trainfeatures)\n",
    "##testing\n",
    "err = 0\n",
    "print 'test on: ',len(testfeatures)\n",
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    if sent != r[1]:\n",
    "       err +=1.\n",
    "print 'error rate: ',err/float(len(testfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on:  400\n",
      "error rate:  0.2825\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 - 800\n",
      "1600\n",
      "test on:  400\n",
      "error rate:  0.2075\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from random import shuffle\n",
    "\n",
    "#train bigram:\n",
    "def bigrams_words_features(words, nbigrams=200,measure=BigramAssocMeasures.chi_sq):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(measure, nbigrams)\n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])\n",
    "\n",
    "negfeatures = [(bigrams_words_features(r.words,500), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(bigrams_words_features(r.words,500), 'pos') for r in reviews_pos]\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)\n",
    "print portionpos,'-',portionneg\n",
    "trainfeatures = negfeatures[:portionpos] + posfeatures[:portionneg]\n",
    "print len(trainfeatures)\n",
    "classifier = NaiveBayesClassifier.train(trainfeatures)\n",
    "##test bigram\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]\n",
    "shuffle(testfeatures)\n",
    "err = 0\n",
    "print 'test on: ',len(testfeatures)\n",
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    #print r[1],'-pred: ',sent\n",
    "    if sent != r[1]:\n",
    "       err +=1.\n",
    "print 'error rate: ',err/float(len(testfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  26060\n"
     ]
    }
   ],
   "source": [
    "import nltk.classify.util, nltk.metrics\n",
    "tot_poswords = [val for l in [r.words for r in reviews_pos] for val in l]\n",
    "tot_negwords = [val for l in [r.words for r in reviews_neg] for val in l]\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "word_fd = FreqDist()\n",
    "label_word_fd = ConditionalFreqDist()\n",
    " \n",
    "for word in tot_poswords:\n",
    "    word_fd[word.lower()] +=1\n",
    "    label_word_fd['pos'][word.lower()] +=1\n",
    " \n",
    "for word in tot_negwords:\n",
    "    word_fd[word.lower()] +=1\n",
    "    label_word_fd['neg'][word.lower()] +=1\n",
    "pos_words = len(tot_poswords)\n",
    "neg_words = len(tot_negwords)\n",
    "\n",
    "tot_words = pos_words + neg_words\n",
    "#select the best words in terms of information contained in the two classes pos and neg\n",
    "word_scores = {}\n",
    " \n",
    "for word, freq in word_fd.iteritems():\n",
    "    pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n",
    "                (freq, pos_words), tot_words)\n",
    "    neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n",
    "                (freq, neg_words), tot_words)\n",
    "    word_scores[word] = pos_score + neg_score\n",
    "print 'total: ',len(word_scores)\n",
    "best = sorted(word_scores.iteritems(), key=lambda (w,s): s, reverse=True)[:10000]\n",
    "bestwords = set([w for w, s in best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 - 800\n",
      "1600\n",
      "test on:  400\n",
      "error rate:  0.13\n"
     ]
    }
   ],
   "source": [
    "#training naive bayes with chi square feature selection of best words\n",
    "def best_words_features(words):\n",
    "    return dict([(word, True) for word in words if word in bestwords])\n",
    "\n",
    "negfeatures = [(best_words_features(r.words), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(best_words_features(r.words), 'pos') for r in reviews_pos]\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)\n",
    "print portionpos,'-',portionneg\n",
    "trainfeatures = negfeatures[:portionpos] + posfeatures[:portionneg]\n",
    "print len(trainfeatures)\n",
    "classifier = NaiveBayesClassifier.train(trainfeatures)\n",
    "##test with feature chi square selection\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]\n",
    "shuffle(testfeatures)\n",
    "err = 0\n",
    "print 'test on: ',len(testfeatures)\n",
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    #print r[1],'-pred: ',sent\n",
    "    if sent != r[1]:\n",
    "       err +=1.\n",
    "print 'error rate: ',err/float(len(testfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "shuffle(tot_reviews)\n",
    "cores = multiprocessing.cpu_count()\n",
    "vec_size = 500\n",
    "model_d2v = Doc2Vec(dm=1, dm_concat=0, size=vec_size, window=5, negative=0, hs=0, min_count=1, workers=cores)\n",
    "\n",
    "#build vocab\n",
    "model_d2v.build_vocab(tot_reviews)\n",
    "#train\n",
    "numepochs= 20\n",
    "for epoch in range(numepochs):\n",
    "    try:\n",
    "        print 'epoch %d' % (epoch)\n",
    "        model_d2v.train(tot_reviews)\n",
    "        model_d2v.alpha *= 0.99\n",
    "        model_d2v.min_alpha = model_d2v.alpha\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split train,test sets\n",
    "trainingsize = 2*int(len(reviews_pos)*0.8)\n",
    "\n",
    "train_d2v = np.zeros((trainingsize, vec_size))\n",
    "train_labels = np.zeros(trainingsize)\n",
    "test_size = len(tot_reviews)-trainingsize\n",
    "test_d2v = np.zeros((test_size, vec_size))\n",
    "test_labels = np.zeros(test_size)\n",
    "\n",
    "cnt_train = 0\n",
    "cnt_test = 0\n",
    "for r in reviews_pos:\n",
    "    name_pos = r.tags[0]\n",
    "    if int(name_pos.split('_')[1])>= int(trainingsize/2.):\n",
    "        test_d2v[cnt_test] = model_d2v.docvecs[name_pos]\n",
    "        test_labels[cnt_test] = 1\n",
    "        cnt_test +=1\n",
    "    else:\n",
    "        train_d2v[cnt_train] = model_d2v.docvecs[name_pos]\n",
    "        train_labels[cnt_train] = 1\n",
    "        cnt_train +=1\n",
    "\n",
    "for r in reviews_neg:\n",
    "    name_neg = r.tags[0]\n",
    "    if int(name_neg.split('_')[1])>= int(trainingsize/2.):\n",
    "        test_d2v[cnt_test] = model_d2v.docvecs[name_neg]\n",
    "        test_labels[cnt_test] = 0\n",
    "        cnt_test +=1\n",
    "    else:\n",
    "        train_d2v[cnt_train] = model_d2v.docvecs[name_neg]       \n",
    "        train_labels[cnt_train] = 0\n",
    "        cnt_train +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5175\n",
      "accuracy: 0.5275\n"
     ]
    }
   ],
   "source": [
    "#train log regre\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_d2v, train_labels)\n",
    "print 'accuracy:',classifier.score(test_d2v,test_labels)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(train_d2v, train_labels)\n",
    "print 'accuracy:',clf.score(test_d2v,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on:  400\n",
      "error rate:  0.1275\n"
     ]
    }
   ],
   "source": [
    "#svm linear\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(train_d2v, train_labels)\n",
    "print clf.score(test_d2v,test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
